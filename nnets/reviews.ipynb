{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. Retrieved from here\n",
    "\n",
    "*    Convulotional NNets\n",
    "     *    dominant approach for recognition & detection tasks - see refs 4, 58, 59, 63-65\n",
    "     *    word vector representations are now widely used in NLP - see refs 14, 17, 72-76\n",
    "     *    use dropout for regularization - see ref 62\n",
    "*    Recurrent NNets\n",
    "     *    advances in architecture and training - see refs 79-82\n",
    "     *    predicting next character or word in a sequence - see refs 75, 83\n",
    "     *    LSTM addresses difficulty maintaining “long” memory - see refs 78,79\n",
    "     *    recent proposals to extend memory \n",
    "          *    Neural Turing Machine - see ref 88\n",
    "          *    Associative memory - see ref 89\n",
    "*    The future\n",
    "     *    systems that combine deep learning & reinforcement learning for classification - see ref 99\n",
    "     *    systems that use RNNs to understand sentences or whole docs - see ref 76, 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " M. Längkvist, L. Karlsson, A. Loutfi, A review of unsupervised feature learning and deep learning for time-series modeling, Pattern Recognit. Lett. 42 (2014) 11–24. Retrieved from here\n",
    "\n",
    "*    Characteristics of time-series data that are challenging\n",
    "     *    noisy\n",
    "     *    high dimensionality \n",
    "     *    no guarantee that the sensor signal contains information necessary to identify target outcome\n",
    "     *    dependence on previous events could require prohibitive input size\n",
    "     *    non-stationary processes - i.e. statistical moments change over time \n",
    "*    Unsupervised feature learning & deep learning methods that have been applied\n",
    "     *    Restricted Boltzmann Machine - standard RBM does not account for time dependence\n",
    "     *    conditional RBM - modificaiton of RBM that uses auto-regressive weights to model short-term temporal structure\n",
    "     *    Auto-encoder NNet - standard AE does not account for time dependence\n",
    "     *    Time-Delay NNet - accounts for time sturcture using convolutions on overlapping input windows\n",
    "     *    Recurrent neural networks - temporal structure built in via self links in hidden units that connects previous output to current input\n",
    "*    Deep learning network: \n",
    "> The goal of a deep network is to build features at the lower layers that will disentangle\n",
    "> the factors of variation in the input data and then combine these representations at \n",
    "> higher layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
