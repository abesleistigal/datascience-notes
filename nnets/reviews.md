# General

## 2015

<a name="LeCun2015"><a> LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. Retrieved from  [here](http://doi.org/10.1038/nature14539)

    *    Convulotional NNets
         *    dominant approach for recognition & detection tasks - see refs 4, 58, 59, 63-65
         *    word vector representations are now widely used in NLP - see refs 14, 17, 72-76
         *    use dropout for regularization - see ref 62
    *    Recurrent NNets
         *    advances in architecture and training - see refs 79-82
         *    predicting next character or word in a sequence - see refs 75, 83
         *    LSTM addresses difficulty maintaining “long” memory - see refs 78,79
         *    recent proposals to extend memory 
              *    Neural Turing Machine - see ref 88
              *    Associative memory - see ref 89
    *    The future
         *    systems that combine deep learning & reinforcement learning for classification - see ref 99
         *    systems that use RNNs to understand sentences or whole docs - see ref 76, 86



